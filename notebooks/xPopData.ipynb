{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XpopSeoul:\n",
    "    def __init__(self, work_dir='.', s3bucket='xpop-seoul'):\n",
    "        self.dataset_list = None\n",
    "        self.prefix = 'local'\n",
    "        self.work_dir = work_dir\n",
    "        self.month = ''\n",
    "        self.dataset_name = ''\n",
    "        self.xpop_zip = ''\n",
    "        self.extract_dir = ''\n",
    "        self.parquet_path = ''\n",
    "        \n",
    "    def list_dataset(self, mode='monthly'):\n",
    "        # file list page\n",
    "        url = \"https://data.seoul.go.kr/dataList/fileView.do?infId=OA-14979&srvType=F\"\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        seq_numbers = list()\n",
    "        for tr in soup.find_all(\"table\", class_=\"dataset01\")[0].find_all(\"tr\"):\n",
    "            try:\n",
    "                tds = tr.find_all(\"td\")\n",
    "                filename = tds[1].text\n",
    "                date = re.search('\\d+', filename).group(0)\n",
    "                href = tds[5].find('a')['href']\n",
    "                seq_number = re.search('\\d+', href).group(0)\n",
    "                seq_numbers.append({'date':date, 'seq_no':seq_number})\n",
    "            except:\n",
    "                pass\n",
    "        df_seq = pd.DataFrame(seq_numbers)\n",
    "        if mode == 'monthly':\n",
    "            df_seq = df_seq[df_seq.date.str.len() == 6]\n",
    "        self.dataset_list = df_seq.reset_index(drop=True)\n",
    "        self.inf_seq = soup.find(\"input\", id=\"infSeq\").get('value')\n",
    "        print(\"number of available datasets: {}\".format(len(df_seq)))\n",
    "        return self\n",
    "\n",
    "    def find_unprocessed(self, s3bucket='xpop-seoul'):\n",
    "        available = self._list_available_dataset()\n",
    "        existing = self._list_parquet_in_s3(s3bucket)\n",
    "        unprocessed = sorted([item.replace(self.prefix + \"-\", \"\") for item in available - existing])\n",
    "        print(\"number of unprocessed datasets: {}\".format(len(unprocessed)))\n",
    "        return unprocessed\n",
    "    \n",
    "    def set_month(self, month):\n",
    "        self.month = month\n",
    "        self.dataset_name = \"{}-{}\".format(self.prefix, month)\n",
    "        self.xpop_zip = os.path.join(self.work_dir, \"{}.zip\".format(self.dataset_name))\n",
    "        self.extract_dir = os.path.join(self.work_dir, self.dataset_name)\n",
    "        self.parquet_path = os.path.join(self.work_dir, \"{}.parquet\".format(self.dataset_name))\n",
    "        return self\n",
    "\n",
    "    def download_csvs(self):\n",
    "        # download\n",
    "        seq_no = self._get_seq_no(self.month)\n",
    "        url = \"http://115.84.165.224/bigfile/iot/inf/nio_download.do\" + \\\n",
    "            \"?&infId=OA-14979&seq={}&infSeq={}\".format(seq_no, self.inf_seq)\n",
    "        urllib.request.urlretrieve(url, self.xpop_zip)\n",
    "        \n",
    "        # extract\n",
    "        if os.path.exists(self.extract_dir):\n",
    "            os.remove(self.extract_dir)\n",
    "        else:\n",
    "            os.mkdir(self.extract_dir)\n",
    "        with zipfile.ZipFile(self.xpop_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(self.extract_dir)\n",
    "        print(\"csv files downloaded at {}\".format(self.extract_dir))\n",
    "        return self\n",
    "    \n",
    "    def make_parquet(self):\n",
    "        csv_files = glob.glob(self.extract_dir + \"/*.csv\")\n",
    "        pop = pd.concat(map(lambda p: pd.read_csv(p, na_values='*'), csv_files))\n",
    "\n",
    "        # rename columns\n",
    "        col_names = list(pop.columns)\n",
    "        new_names = list()\n",
    "        for col in col_names:\n",
    "            col = col.replace('남자', 'xpop_m')\n",
    "            col = col.replace('여자', 'xpop_f')\n",
    "            col = col.replace('세부터', 'to')\n",
    "            col = col.replace('세생활인구수', '')\n",
    "            col = col.replace('세이상생활인구수', 'over')\n",
    "            col = col.replace('기준일ID', 'date')\n",
    "            col = col.replace('시간대구분', 'hour')\n",
    "            col = col.replace('행정동코드', 'adm_id')\n",
    "            col = col.replace('집계구코드', 'census_id')\n",
    "            col = col.replace('총생활인구수', 'xpop_total')\n",
    "            new_names.append(col)\n",
    "        pop.columns = new_names\n",
    "\n",
    "        pop = pop.fillna(0)\n",
    "        pop['date'] = pd.to_datetime(pop['date'], format='%Y%m%d')\n",
    "\n",
    "        # change dtype\n",
    "        for col in pop.columns:\n",
    "            if 'xpop' in col or 'hour' in col:\n",
    "                pop[col] = pop[col].astype(int)\n",
    "            elif 'date' in col:\n",
    "                pass\n",
    "            else:\n",
    "                pop[col] = pop[col].astype(str)\n",
    "            \n",
    "        pop.to_parquet(self.parquet_path)\n",
    "        print(\"parquet create at {}\".format(self.parquet_path))\n",
    "        return self\n",
    "    \n",
    "    def s3upload_parquet(self, s3bucket='xpop-seoul', profile='default'):\n",
    "        session = boto3.session.Session(profile_name=profile)\n",
    "        s3 = session.resource('s3')\n",
    "        s3.meta.client.upload_file(self.parquet_path, s3bucket, \"{}.parquet\".format(self.dataset_name))\n",
    "        print(\"parquet uploaded to {}\".format(s3bucket))\n",
    "        return self\n",
    "\n",
    "    def clean_up(self):\n",
    "        os.remove(self.xpop_zip)\n",
    "        shutil.rmtree(self.extract_dir)\n",
    "        os.remove(self.parquet_path)\n",
    "        return self\n",
    "    \n",
    "    def _get_seq_no(self, month):\n",
    "        return self.dataset_list[self.dataset_list.date==month]['seq_no'].values[0]\n",
    "    \n",
    "    def _list_parquet_in_s3(self, s3bucket):\n",
    "        session = boto3.session.Session(profile_name='lambda')\n",
    "        s3 = session.resource('s3')\n",
    "        bucket = s3.Bucket(s3bucket)\n",
    "        parquets = set()\n",
    "        for s3_file in bucket.objects.all():\n",
    "            parquets.add(s3_file.key.replace(\".parquet\", \"\"))\n",
    "        return parquets\n",
    "    \n",
    "    def _list_available_dataset(self):\n",
    "        return set(self.prefix + \"-\" + self.dataset_list['date'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Merge/Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 21\n",
      "csv files downloaded at ../data/interim/local-201703\n",
      "parquet create at ../data/interim/local-201703.parquet\n",
      "parquet uploaded to xpop-seoul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XpopSeoul at 0x137c14978>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpopSeoul(work_dir=\"../data/interim\")\\\n",
    ".list_dataset()\\\n",
    ".set_month('201703')\\\n",
    ".download_csvs()\\\n",
    ".make_parquet()\\\n",
    ".s3upload_parquet(s3bucket='xpop-seoul', profile='lambda')\\\n",
    ".clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List datasets not uploaded in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 21\n",
      "number of unprocessed datasets: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['201704',\n",
       " '201705',\n",
       " '201706',\n",
       " '201707',\n",
       " '201708',\n",
       " '201709',\n",
       " '201710',\n",
       " '201711',\n",
       " '201712',\n",
       " '201801',\n",
       " '201802',\n",
       " '201803',\n",
       " '201804',\n",
       " '201807',\n",
       " '201808',\n",
       " '201809']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpopSeoul()\\\n",
    ".list_dataset()\\\n",
    ".find_unprocessed(s3bucket='xpop-seoul')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
