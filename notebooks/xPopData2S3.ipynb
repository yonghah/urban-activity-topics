{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The city of Seoul maintains [datasets for estimated hourly population](https://data.seoul.go.kr/dataVisual/seoul/seoulLivingPopulation.do). \n",
    "A zipped file for this dataset is uploaded monthly and the zip file \n",
    "has around 30 csv file for daily data inside it. In this notebook,\n",
    "we are going to \n",
    "\n",
    "1. unzip this monthly dataset\n",
    "2. merge by month \n",
    "3. change column names into English\n",
    "4. convert the monthly dataframe into parquet format\n",
    "5. upload the parquets to S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XpopSeoul:\n",
    "    def __init__(self, work_dir='.', s3bucket='xpop-seoul'):\n",
    "        self.dataset_list = None\n",
    "        self.prefix = 'local'\n",
    "        self.work_dir = work_dir\n",
    "        self.month = ''\n",
    "        self.dataset_name = ''\n",
    "        self.xpop_zip = ''\n",
    "        self.extract_dir = ''\n",
    "        self.parquet_path = ''\n",
    "        \n",
    "    def list_dataset(self, mode='monthly'):\n",
    "        # file list page\n",
    "        url = \"https://data.seoul.go.kr/dataList/fileView.do?infId=OA-14979&srvType=F\"\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        seq_numbers = list()\n",
    "        for tr in soup.find_all(\"table\", class_=\"dataset01\")[0].find_all(\"tr\"):\n",
    "            try:\n",
    "                tds = tr.find_all(\"td\")\n",
    "                filename = tds[1].text\n",
    "                date = re.search('\\d+', filename).group(0)\n",
    "                href = tds[5].find('a')['href']\n",
    "                seq_number = re.search('\\d+', href).group(0)\n",
    "                seq_numbers.append({'date':date, 'seq_no':seq_number})\n",
    "            except:\n",
    "                pass\n",
    "        df_seq = pd.DataFrame(seq_numbers)\n",
    "        if mode == 'monthly':\n",
    "            df_seq = df_seq[df_seq.date.str.len() == 6]\n",
    "        self.dataset_list = df_seq.reset_index(drop=True)\n",
    "        self.inf_seq = soup.find(\"input\", id=\"infSeq\").get('value')\n",
    "        print(\"number of available datasets: {}\".format(len(df_seq)))\n",
    "        return self\n",
    "\n",
    "    def find_unprocessed(self, s3bucket='xpop-seoul', profile='default'):\n",
    "        available = self._list_available_dataset()\n",
    "        existing = self._list_parquet_in_s3(s3bucket, profile)\n",
    "        unprocessed = sorted([item.replace(self.prefix + \"-\", \"\") for item in available - existing])\n",
    "        print(\"number of unprocessed datasets: {}\".format(len(unprocessed)))\n",
    "        return unprocessed\n",
    "    \n",
    "    def set_month(self, month):\n",
    "        self.month = month\n",
    "        self.dataset_name = \"{}-{}\".format(self.prefix, month)\n",
    "        self.xpop_zip = os.path.join(self.work_dir, \"{}.zip\".format(self.dataset_name))\n",
    "        self.extract_dir = os.path.join(self.work_dir, self.dataset_name)\n",
    "        self.parquet_path = os.path.join(self.work_dir, \"{}.parquet\".format(self.dataset_name))\n",
    "        return self\n",
    "\n",
    "    def download_csvs(self):\n",
    "        # download\n",
    "        seq_no = self._get_seq_no(self.month)\n",
    "        url = \"http://115.84.165.224/bigfile/iot/inf/nio_download.do\" + \\\n",
    "            \"?&infId=OA-14979&seq={}&infSeq={}\".format(seq_no, self.inf_seq)\n",
    "        urllib.request.urlretrieve(url, self.xpop_zip)\n",
    "        \n",
    "        # extract\n",
    "        if os.path.exists(self.extract_dir):\n",
    "            os.remove(self.extract_dir)\n",
    "        else:\n",
    "            os.mkdir(self.extract_dir)\n",
    "        with zipfile.ZipFile(self.xpop_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(self.extract_dir)\n",
    "            \n",
    "        # sometimes zip files are zipped\n",
    "        zip_files = glob.glob(self.extract_dir + \"/*.zip\")\n",
    "        for zip_file in zip_files:\n",
    "            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(self.extract_dir)\n",
    "        \n",
    "        print(\"csv files downloaded at {}\".format(self.extract_dir))\n",
    "        return self\n",
    "    \n",
    "    def make_parquet(self):\n",
    "        csv_files = glob.glob(self.extract_dir + \"/*.csv\")\n",
    "        dfs = list()\n",
    "        for csv in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv, na_values='*', encoding = \"euc_kr\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv, na_values='*')\n",
    "            df = self.rename_columns(df)\n",
    "            dfs.append(df)\n",
    "        pop = pd.concat(dfs)\n",
    "        # pop = pd.concat(map(lambda p: pd.read_csv(p, na_values='*', encoding = \"euc_kr\"), csv_files))\n",
    "\n",
    "        pop = pop.fillna(0)\n",
    "        pop['date'] = pd.to_datetime(pop['date'], format='%Y%m%d')\n",
    "\n",
    "        # change dtype\n",
    "        for col in pop.columns:\n",
    "            if 'xpop' in col or 'hour' in col:\n",
    "                pop[col] = pop[col].astype(int)\n",
    "            elif 'date' in col:\n",
    "                pass\n",
    "            else:\n",
    "                pop[col] = pop[col].astype(str)\n",
    "            \n",
    "        pop.to_parquet(self.parquet_path)\n",
    "        print(\"parquet create at {}\".format(self.parquet_path))\n",
    "        return self\n",
    "    \n",
    "    def rename_columns(self, df):\n",
    "        # rename columns\n",
    "        col_names = list(df.columns)\n",
    "        new_names = list()\n",
    "        for col in df.columns:\n",
    "            col = col.replace('남자', 'xpop_m')\n",
    "            col = col.replace('여자', 'xpop_f')\n",
    "            col = col.replace('세부터', 'to')\n",
    "            col = col.replace('세생활인구수', '')\n",
    "            col = col.replace('세이상생활인구수', 'over')\n",
    "            col = col.replace('기준일ID', 'date')\n",
    "            col = col.replace('\\\"', '')\n",
    "            col = col.replace('?', '')\n",
    "            col = col.replace('시간대구분', 'hour')\n",
    "            col = col.replace('행정동코드', 'adm_id')\n",
    "            col = col.replace('집계구코드', 'census_id')\n",
    "            col = col.replace('총생활인구수', 'xpop_total')\n",
    "            new_names.append(col)\n",
    "        df.columns = new_names\n",
    "        return df\n",
    "    \n",
    "    def s3upload_parquet(self, s3bucket='xpop-seoul', profile='default'):\n",
    "        session = boto3.session.Session(profile_name=profile)\n",
    "        s3 = session.resource('s3')\n",
    "        s3.meta.client.upload_file(self.parquet_path, s3bucket, \"monthly/{}.parquet\".format(self.dataset_name))\n",
    "        print(\"parquet uploaded to {}\".format(s3bucket))\n",
    "        return self\n",
    "\n",
    "    def clean_up(self):\n",
    "        os.remove(self.xpop_zip)\n",
    "        shutil.rmtree(self.extract_dir)\n",
    "        os.remove(self.parquet_path)\n",
    "        return self\n",
    "    \n",
    "    def _get_seq_no(self, month):\n",
    "        return self.dataset_list[self.dataset_list.date==month]['seq_no'].values[0]\n",
    "    \n",
    "    def _list_parquet_in_s3(self, s3bucket, profile):\n",
    "        session = boto3.session.Session(profile_name=profile)\n",
    "        s3 = session.resource('s3')\n",
    "        bucket = s3.Bucket(s3bucket)\n",
    "        parquets = set()\n",
    "        for s3_file in bucket.objects.all():\n",
    "            if \"monthly/\" in s3_file.key and \".parquet\" in s3_file.key:\n",
    "                name = s3_file.key.replace(\".parquet\", \"\").replace(\"monthly/\", \"\")\n",
    "                parquets.add(name)\n",
    "        return parquets\n",
    "    \n",
    "    def _list_available_dataset(self):\n",
    "        return set(self.prefix + \"-\" + self.dataset_list['date'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Merge/Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 38\n",
      "csv files downloaded at ../data/interim/local-202002\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb1 in position 2: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3a354ff5a38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mXpopSeoul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/interim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlist_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mset_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'202002'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload_csvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmake_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-821b86942df3>\u001b[0m in \u001b[0;36mmake_parquet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-821b86942df3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/ped-topic/env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb1 in position 2: invalid start byte"
     ]
    }
   ],
   "source": [
    "XpopSeoul(work_dir=\"../data/interim\")\\\n",
    ".list_dataset()\\\n",
    ".set_month('202002')\\\n",
    ".download_csvs()\\\n",
    ".make_parquet()\\\n",
    ".s3upload_parquet(s3bucket='xpop-seoul', profile='lambda')\\\n",
    ".clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update datasets not uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 21\n",
      "number of unprocessed datasets: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "updater = XpopSeoul(work_dir=\"../data/interim\")\\\n",
    ".list_dataset()\n",
    "\n",
    "# list datasetss not in S3 bucket\n",
    "update_list = updater.find_unprocessed(s3bucket='xpop-seoul',  profile='lambda')\n",
    "print(update_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv files downloaded at ../data/interim/local-201808\n",
      "parquet create at ../data/interim/local-201808.parquet\n",
      "parquet uploaded to xpop-seoul\n",
      "csv files downloaded at ../data/interim/local-201809\n",
      "parquet create at ../data/interim/local-201809.parquet\n",
      "parquet uploaded to xpop-seoul\n"
     ]
    }
   ],
   "source": [
    "for month in update_list:\n",
    "    updater\\\n",
    "    .set_month(month)\\\n",
    "    .download_csvs()\\\n",
    "    .make_parquet()\\\n",
    "    .s3upload_parquet(s3bucket='xpop-seoul', profile='lambda')\\\n",
    "    .clean_up()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
