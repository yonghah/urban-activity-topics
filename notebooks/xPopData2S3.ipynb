{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The city of Seoul maintains [datasets for estimated hourly population](https://data.seoul.go.kr/dataVisual/seoul/seoulLivingPopulation.do). \n",
    "A zipped file for this dataset is uploaded monthly and the zip file \n",
    "has around 30 csv file for daily data inside it. In this notebook,\n",
    "we are going to \n",
    "\n",
    "1. unzip this monthly dataset\n",
    "2. merge by month \n",
    "3. change column names into English\n",
    "4. convert the monthly dataframe into parquet format\n",
    "5. upload the parquets to S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XpopSeoul:\n",
    "    def __init__(self, work_dir='.', s3bucket='xpop-seoul'):\n",
    "        self.dataset_list = None\n",
    "        self.prefix = 'local'\n",
    "        self.work_dir = work_dir\n",
    "        self.month = ''\n",
    "        self.dataset_name = ''\n",
    "        self.xpop_zip = ''\n",
    "        self.extract_dir = ''\n",
    "        self.parquet_path = ''\n",
    "        \n",
    "    def list_dataset(self, mode='monthly'):\n",
    "        # file list page\n",
    "        url = \"https://data.seoul.go.kr/dataList/fileView.do?infId=OA-14979&srvType=F\"\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        seq_numbers = list()\n",
    "        for tr in soup.find_all(\"table\", class_=\"dataset01\")[0].find_all(\"tr\"):\n",
    "            try:\n",
    "                tds = tr.find_all(\"td\")\n",
    "                filename = tds[1].text\n",
    "                date = re.search('\\d+', filename).group(0)\n",
    "                href = tds[5].find('a')['href']\n",
    "                seq_number = re.search('\\d+', href).group(0)\n",
    "                seq_numbers.append({'date':date, 'seq_no':seq_number})\n",
    "            except:\n",
    "                pass\n",
    "        df_seq = pd.DataFrame(seq_numbers)\n",
    "        if mode == 'monthly':\n",
    "            df_seq = df_seq[df_seq.date.str.len() == 6]\n",
    "        self.dataset_list = df_seq.reset_index(drop=True)\n",
    "        self.inf_seq = soup.find(\"input\", id=\"infSeq\").get('value')\n",
    "        print(\"number of available datasets: {}\".format(len(df_seq)))\n",
    "        return self\n",
    "\n",
    "    def find_unprocessed(self, s3bucket='xpop-seoul', profile='default'):\n",
    "        available = self._list_available_dataset()\n",
    "        existing = self._list_parquet_in_s3(s3bucket, profile)\n",
    "        unprocessed = sorted([item.replace(self.prefix + \"-\", \"\") for item in available - existing])\n",
    "        print(\"number of unprocessed datasets: {}\".format(len(unprocessed)))\n",
    "        return unprocessed\n",
    "    \n",
    "    def set_month(self, month):\n",
    "        self.month = month\n",
    "        self.dataset_name = \"{}-{}\".format(self.prefix, month)\n",
    "        self.xpop_zip = os.path.join(self.work_dir, \"{}.zip\".format(self.dataset_name))\n",
    "        self.extract_dir = os.path.join(self.work_dir, self.dataset_name)\n",
    "        self.parquet_path = os.path.join(self.work_dir, \"{}.parquet\".format(self.dataset_name))\n",
    "        return self\n",
    "\n",
    "    def download_csvs(self):\n",
    "        # download\n",
    "        seq_no = self._get_seq_no(self.month)\n",
    "        url = \"http://115.84.165.224/bigfile/iot/inf/nio_download.do\" + \\\n",
    "            \"?&infId=OA-14979&seq={}&infSeq={}\".format(seq_no, self.inf_seq)\n",
    "        urllib.request.urlretrieve(url, self.xpop_zip)\n",
    "        \n",
    "        # extract\n",
    "        if os.path.exists(self.extract_dir):\n",
    "            os.remove(self.extract_dir)\n",
    "        else:\n",
    "            os.mkdir(self.extract_dir)\n",
    "        with zipfile.ZipFile(self.xpop_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(self.extract_dir)\n",
    "            \n",
    "        # sometimes zip files are zipped\n",
    "        zip_files = glob.glob(self.extract_dir + \"/*.zip\")\n",
    "        for zip_file in zip_files:\n",
    "            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(self.extract_dir)\n",
    "        \n",
    "        print(\"csv files downloaded at {}\".format(self.extract_dir))\n",
    "        return self\n",
    "    \n",
    "    def make_parquet(self):\n",
    "        csv_files = glob.glob(self.extract_dir + \"/*.csv\")\n",
    "        dfs = list()\n",
    "        for csv in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv, na_values='*', encoding = \"euc_kr\")\n",
    "                if isinstance(df.index, pd.MultiIndex):\n",
    "                    dft = df.reset_index().iloc[:,0:len(df.columns)]\n",
    "                    dft.columns = df.columns\n",
    "                    df = dft\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv, na_values='*')\n",
    "            df = self.rename_columns(df)\n",
    "            dfs.append(df)\n",
    "        pop = pd.concat(dfs)\n",
    "        # pop = pd.concat(map(lambda p: pd.read_csv(p, na_values='*', encoding = \"euc_kr\"), csv_files))\n",
    "\n",
    "        pop = pop.fillna(0)\n",
    "        pop['date'] = pd.to_datetime(pop['date'], format='%Y%m%d')\n",
    "\n",
    "        # change dtype\n",
    "        for col in pop.columns:\n",
    "            if 'xpop' in col or 'hour' in col:\n",
    "                pop[col] = pop[col].astype(int)\n",
    "            elif 'date' in col:\n",
    "                pass\n",
    "            else:\n",
    "                pop[col] = pop[col].astype(str)\n",
    "            \n",
    "        pop.to_parquet(self.parquet_path)\n",
    "        print(\"parquet create at {}\".format(self.parquet_path))\n",
    "        return self\n",
    "    \n",
    "    def rename_columns(self, df):\n",
    "        # rename columns\n",
    "        col_names = list(df.columns)\n",
    "        new_names = list()\n",
    "        for col in df.columns:\n",
    "            col = col.replace('남자', 'xpop_m')\n",
    "            col = col.replace('여자', 'xpop_f')\n",
    "            col = col.replace('세부터', 'to')\n",
    "            col = col.replace('세생활인구수', '')\n",
    "            col = col.replace('세이상생활인구수', 'over')\n",
    "            col = col.replace('기준일ID', 'date')\n",
    "            col = col.replace('\\\"', '')\n",
    "            col = col.replace('?', '')\n",
    "            col = col.replace('시간대구분', 'hour')\n",
    "            col = col.replace('행정동코드', 'adm_id')\n",
    "            col = col.replace('집계구코드', 'census_id')\n",
    "            col = col.replace('총생활인구수', 'xpop_total')\n",
    "            new_names.append(col)\n",
    "        df.columns = new_names\n",
    "        return df\n",
    "    \n",
    "    def s3upload_parquet(self, s3bucket='xpop-seoul', profile='default'):\n",
    "        session = boto3.session.Session(profile_name=profile)\n",
    "        s3 = session.resource('s3')\n",
    "        s3.meta.client.upload_file(self.parquet_path, s3bucket, \"monthly/{}.parquet\".format(self.dataset_name))\n",
    "        print(\"parquet uploaded to {}\".format(s3bucket))\n",
    "        return self\n",
    "\n",
    "    def clean_up(self):\n",
    "        os.remove(self.xpop_zip)\n",
    "        shutil.rmtree(self.extract_dir)\n",
    "        os.remove(self.parquet_path)\n",
    "        return self\n",
    "    \n",
    "    def _get_seq_no(self, month):\n",
    "        return self.dataset_list[self.dataset_list.date==month]['seq_no'].values[0]\n",
    "    \n",
    "    def _list_parquet_in_s3(self, s3bucket, profile):\n",
    "        session = boto3.session.Session(profile_name=profile)\n",
    "        s3 = session.resource('s3')\n",
    "        bucket = s3.Bucket(s3bucket)\n",
    "        parquets = set()\n",
    "        for s3_file in bucket.objects.all():\n",
    "            if \"monthly/\" in s3_file.key and \".parquet\" in s3_file.key:\n",
    "                name = s3_file.key.replace(\".parquet\", \"\").replace(\"monthly/\", \"\")\n",
    "                parquets.add(name)\n",
    "        return parquets\n",
    "    \n",
    "    def _list_available_dataset(self):\n",
    "        return set(self.prefix + \"-\" + self.dataset_list['date'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Merge/Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 38\n",
      "csv files downloaded at ../data/interim/local-202001\n",
      "parquet create at ../data/interim/local-202001.parquet\n",
      "parquet uploaded to xpop-seoul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XpopSeoul at 0x7fd4c4e42710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpopSeoul(work_dir=\"../data/interim\")\\\n",
    ".list_dataset()\\\n",
    ".set_month('202001')\\\n",
    ".download_csvs()\\\n",
    ".make_parquet()\\\n",
    ".s3upload_parquet(s3bucket='xpop-seoul')\\\n",
    ".clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update datasets not uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available datasets: 38\n",
      "number of unprocessed datasets: 3\n",
      "['201910', '201911', '201912']\n"
     ]
    }
   ],
   "source": [
    "updater = XpopSeoul(work_dir=\"../data/interim\")\\\n",
    ".list_dataset()\n",
    "\n",
    "# list datasetss not in S3 bucket\n",
    "update_list = updater.find_unprocessed(s3bucket='xpop-seoul')\n",
    "print(update_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet create at ../data/interim/local-201909.parquet\n",
      "parquet uploaded to xpop-seoul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XpopSeoul at 0x7fd535f80710>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updater\\\n",
    "    .set_month('201909')\\\n",
    "    .make_parquet()\\\n",
    "    .s3upload_parquet(s3bucket='xpop-seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv files downloaded at ../data/interim/local-201910\n",
      "parquet create at ../data/interim/local-201910.parquet\n",
      "parquet uploaded to xpop-seoul\n",
      "csv files downloaded at ../data/interim/local-201911\n",
      "parquet create at ../data/interim/local-201911.parquet\n",
      "parquet uploaded to xpop-seoul\n",
      "csv files downloaded at ../data/interim/local-201912\n",
      "parquet create at ../data/interim/local-201912.parquet\n",
      "parquet uploaded to xpop-seoul\n"
     ]
    }
   ],
   "source": [
    "for month in update_list:\n",
    "    updater\\\n",
    "    .set_month(month)\\\n",
    "    .download_csvs()\\\n",
    "    .make_parquet()\\\n",
    "    .s3upload_parquet(s3bucket='xpop-seoul')\\\n",
    "    .clean_up()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
